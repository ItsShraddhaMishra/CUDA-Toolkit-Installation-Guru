 __Terminal__: nvidia-smi
    * The CUDA might return some wrong cuda (The latest release) version installed version. May be.


    * This does not mean you did something wrong. This happens due to the installed cuda toolkit installs its own new release drivers which overwrites any old version. Hence we didn't installed nvidia drivers as step 1.

 __Terminal__: nvcc -V
    * The terminal returns correct installed CUDA version with the version detail of the NVIDIA driver. This indicates we have installed correct CUDA Toolkit version, however wrong driver version got installed with it which is why in *nvidia-smi* command, it shows latest version of CUDA Toolkit (that has been released. But not installed. Although available to be upgraded.)


# Why we need this step?

    * Difference in CUDA version's impact is displayed when You run the tensorflow. The Tensorflow will look for different library files and CUDA will return different, hence You will not be able to initiate GPU device correctly when you run your Program. Even if you install correct CUDNN files.


### Dont worry! No need to Purge and reinstall again! Just follow 3 steps:

1. __Terminal__: ubuntu-drivers devices
    * Choose the one from GPU deployment Management Guide. https://docs.nvidia.com/deploy/cuda-compatibility/index.html 

    * I chose #450


2. __Terminal__: sudo apt-get install nvidia-driver-450
        >>Yes
        >>sudo reboot

#### After reboot:

 __Terminal__: nvidia-smi

 Voila! done. Go train Your data!

 # Note: 
 If you have drivers installed as Step 1, they will be overwritten by Step 2. IN THAT CASE, YOU CAN STILL DO STEP 3. Followed by reboot.
